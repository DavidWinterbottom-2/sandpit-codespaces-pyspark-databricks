{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e1748b3",
   "metadata": {},
   "source": [
    "# Remote Spark Connection Example\n",
    "\n",
    "This notebook demonstrates how to connect to a remote Databricks cluster using Databricks Connect.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Databricks Workspace**: You need access to a Databricks workspace\n",
    "2. **Personal Access Token**: Generate a personal access token from your Databricks workspace\n",
    "3. **Cluster**: Have a running Databricks cluster (or specify cluster ID for auto-start)\n",
    "4. **Environment**: Use either `delta-lake` or `unity-catalog` conda environment\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run the configuration script to set up your credentials:\n",
    "```bash\n",
    "./scripts/configure-remote.sh\n",
    "```\n",
    "\n",
    "Or manually create a `.env` file with:\n",
    "```\n",
    "DATABRICKS_HOST=https://your-workspace.databricks.com\n",
    "DATABRICKS_TOKEN=your-personal-access-token\n",
    "DATABRICKS_CLUSTER_ID=your-cluster-id  # Optional\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e89681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from shared.spark_utils import SparkSessionManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65111663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check configuration\n",
    "databricks_host = os.getenv(\"DATABRICKS_HOST\")\n",
    "databricks_token = os.getenv(\"DATABRICKS_TOKEN\")\n",
    "cluster_id = os.getenv(\"DATABRICKS_CLUSTER_ID\")\n",
    "\n",
    "print(f\"Databricks Host: {databricks_host}\")\n",
    "print(f\"Token configured: {'Yes' if databricks_token else 'No'}\")\n",
    "print(f\"Cluster ID: {cluster_id if cluster_id else 'Not specified'}\")\n",
    "\n",
    "if not databricks_host or not databricks_token:\n",
    "    print(\"\\n‚ùå Error: DATABRICKS_HOST and DATABRICKS_TOKEN must be configured!\")\n",
    "    print(\"Please run: ./scripts/configure-remote.sh\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Configuration looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf44266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to remote Spark cluster\n",
    "print(\"üöÄ Connecting to remote Databricks cluster...\")\n",
    "\n",
    "try:\n",
    "    spark = SparkSessionManager.get_session(\"remote\", \"Remote-Jupyter-Example\")\n",
    "    print(\"‚úÖ Successfully connected!\")\n",
    "    print(f\"   Spark Version: {spark.version}\")\n",
    "    print(f\"   Application ID: {spark.sparkContext.applicationId}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Verify your credentials are correct\")\n",
    "    print(\"2. Ensure your Databricks cluster is running\")\n",
    "    print(\"3. Check network connectivity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6136401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic functionality\n",
    "print(\"üìä Testing basic Spark functionality...\")\n",
    "\n",
    "# Simple test query\n",
    "df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        'Hello from Remote Spark!' as message,\n",
    "        current_timestamp() as timestamp,\n",
    "        version() as spark_version\n",
    "\"\"\")\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f3656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the environment\n",
    "print(\"üóÑÔ∏è Exploring available databases...\")\n",
    "\n",
    "# Show databases\n",
    "databases_df = spark.sql(\"SHOW DATABASES\")\n",
    "databases_df.show()\n",
    "\n",
    "# Get database list as Python list\n",
    "databases = [row.namespace for row in databases_df.collect()]\n",
    "print(f\"\\nFound {len(databases)} databases: {', '.join(databases)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f430d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and work with a DataFrame\n",
    "print(\"üìà Creating sample DataFrame...\")\n",
    "\n",
    "# Create sample data\n",
    "sample_data = [\n",
    "    (1, \"Alice\", 25, \"Engineering\"),\n",
    "    (2, \"Bob\", 30, \"Marketing\"),\n",
    "    (3, \"Charlie\", 35, \"Engineering\"),\n",
    "    (4, \"Diana\", 28, \"Sales\"),\n",
    "    (5, \"Eve\", 32, \"Engineering\")\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"department\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(sample_data, columns)\n",
    "\n",
    "print(\"\\nüìã Sample data:\")\n",
    "df.show()\n",
    "\n",
    "print(\"\\nüìä Department statistics:\")\n",
    "df.groupBy(\"department\").agg(\n",
    "    {\"age\": \"avg\", \"id\": \"count\"}\n",
    ").withColumnRenamed(\"avg(age)\", \"avg_age\").withColumnRenamed(\"count(id)\", \"employee_count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f4be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Work with Delta Lake (if available)\n",
    "print(\"üî∫ Testing Delta Lake functionality...\")\n",
    "\n",
    "try:\n",
    "    # Try to create a temporary Delta table\n",
    "    temp_table_path = \"/tmp/delta-table-test\"\n",
    "    \n",
    "    # Write as Delta format\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(temp_table_path)\n",
    "    \n",
    "    # Read it back\n",
    "    delta_df = spark.read.format(\"delta\").load(temp_table_path)\n",
    "    \n",
    "    print(\"‚úÖ Delta Lake is working!\")\n",
    "    print(\"\\nüìä Data from Delta table:\")\n",
    "    delta_df.show()\n",
    "    \n",
    "    # Clean up\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS delta.`{temp_table_path}`\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Delta Lake test failed: {e}\")\n",
    "    print(\"This might be expected if Delta Lake is not configured in your cluster.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee20f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "print(\"üîÑ Cleaning up...\")\n",
    "SparkSessionManager.stop_session()\n",
    "print(\"‚úÖ Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
