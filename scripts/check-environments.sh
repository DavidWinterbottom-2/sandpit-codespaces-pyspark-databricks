#!/bin/bash

# Check status of all conda environments
echo "ğŸ” Checking PySpark environment status..."

# Check if conda is available
if ! command -v conda &> /dev/null; then
    echo "âŒ Conda is not available."
    exit 1
fi

echo "ğŸ“¦ Conda version:"
conda --version
echo ""

echo "ğŸŒ Available conda environments:"
conda env list
echo ""

# Check each required environment
environments=("base-pyspark" "streaming" "delta-lake" "unity-catalog")
echo "ğŸ”§ PySpark environment status:"

for env in "${environments[@]}"; do
    echo -n "   $env: "
    if conda env list | grep -q "$env"; then
        echo "âœ… EXISTS"
        
        # Check Python and key packages
        source "$(conda info --base)/etc/profile.d/conda.sh"
        conda activate "$env" 2>/dev/null
        
        if [[ $? -eq 0 ]]; then
            python_version=$(python --version 2>&1)
            spark_version=$(python -c "import pyspark; print(pyspark.__version__)" 2>/dev/null || echo "Not installed")
            echo "      Python: $python_version"
            echo "      PySpark: $spark_version"
            
            # Check environment-specific packages
            case $env in
                "streaming")
                    kafka_version=$(python -c "import kafka; print(kafka.__version__)" 2>/dev/null || echo "Not installed")
                    echo "      Kafka: $kafka_version"
                    ;;
                "delta-lake")
                    delta_version=$(python -c "import delta; print('Installed')" 2>/dev/null || echo "Not installed")
                    echo "      Delta Lake: $delta_version"
                    ;;
                "unity-catalog")
                    databricks_version=$(python -c "import databricks; print('Installed')" 2>/dev/null || echo "Not installed")
                    echo "      Databricks SDK: $databricks_version"
                    ;;
            esac
        else
            echo "      âŒ Failed to activate environment"
        fi
        echo ""
    else
        echo "âŒ MISSING"
    fi
done

# Check Java (required for Spark)
echo "â˜• Java status:"
if command -v java &> /dev/null; then
    java_version=$(java -version 2>&1 | head -n 1)
    echo "   $java_version"
    
    if java -version 2>&1 | grep -q "11\|17\|21"; then
        echo "   âœ… Compatible Java version"
    else
        echo "   âš ï¸  May not be compatible with Spark (recommended: Java 11, 17, or 21)"
    fi
else
    echo "   âŒ Java not found"
fi
echo ""

# Check port availability
echo "ğŸŒ Port availability:"
ports=(4040 8080 8888 9092)
for port in "${ports[@]}"; do
    if lsof -i :$port &> /dev/null; then
        echo "   Port $port: âŒ IN USE"
    else
        echo "   Port $port: âœ… AVAILABLE"
    fi
done
echo ""

# Check disk space
echo "ğŸ’¾ Disk space:"
df -h . | tail -1 | awk '{print "   Available: " $4 " (" $5 " used)"}'
echo ""

# Summary
echo "ğŸ“‹ Summary:"
missing_envs=0
for env in "${environments[@]}"; do
    if ! conda env list | grep -q "$env"; then
        ((missing_envs++))
    fi
done

if [[ $missing_envs -eq 0 ]]; then
    echo "   âœ… All PySpark environments are ready!"
    echo "   ğŸš€ Run './scripts/run-all-examples.sh' to test everything"
else
    echo "   âŒ $missing_envs environment(s) missing"
    echo "   ğŸ”§ Run './scripts/setup-environments.sh' to create them"
fi